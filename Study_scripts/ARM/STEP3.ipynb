{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import signal\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_list(strings, target):\n",
    "    result = []\n",
    "    for string in strings:\n",
    "        if target in string:\n",
    "            result.append(string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function call timed out\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException(\"Function timed out\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "def itemset(df,thresholds):\n",
    "    frequent_itemsets = None\n",
    "    for threshold in thresholds:\n",
    "        signal.alarm(90)  # set timeout to 60 seconds\n",
    "        try:\n",
    "            frequent_itemsets = apriori(df, min_support=threshold, use_colnames=True)\n",
    "        except TimeoutException:\n",
    "            print(f\"Method did not return results in 1 minute. Stopping and recalling with threshold = {threshold}\")\n",
    "            continue\n",
    "        signal.alarm(0)  # clear the alarm\n",
    "        break\n",
    "        \n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import os\n",
    "import csv\n",
    "\n",
    "p='/Projects'\n",
    "dirlist = [item for item in os.listdir(p) if os.path.isdir(os.path.join(p, item))]\n",
    "\n",
    "lenth = len(dirlist)\n",
    "\n",
    "project='ansible-collection-hardening'\n",
    "ProjectClone= p + \"/\" + project\n",
    "    \n",
    "try:\n",
    "            with open(ProjectClone+'/_90_train_Of_Commits.csv','r') as read_obj:\n",
    "                        csv_reader = csv.reader(read_obj)\n",
    "\n",
    "                  # convert string to list\n",
    "                        list_of_files = list(csv_reader)\n",
    "                        final_list_of_files = [item for sublist in list_of_files for item in sublist]\n",
    "                        te = TransactionEncoder()\n",
    "                        te_ary = te.fit(list_of_files).transform(list_of_files)\n",
    "                        df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "                        \n",
    "                        frequent_itemsets = apriori(df, min_support=0.08, use_colnames=True)\n",
    "                        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.01)\n",
    "                        target = '_(IaC)_'\n",
    "                        other = '_(Other)_'\n",
    "                        \n",
    "                        df = pd.DataFrame(rules)\n",
    "                        counter = 1\n",
    "                        df2 = pd.DataFrame()\n",
    "                        single_antecedent_rules = rules[rules['antecedents'].apply(lambda x: len(x) == 1)]\n",
    "                        single_consequents_rules= single_antecedent_rules[single_antecedent_rules['consequents'].apply(lambda x: len(x) == 1)]\n",
    "                        \n",
    "                        new_rules = []\n",
    "                        for index, rule in single_consequents_rules.iterrows():\n",
    "\n",
    "                              antecedents = rule[\"antecedents\"]\n",
    "                              \n",
    "                              consequent = rule['consequents']\n",
    "                              if len(str(antecedents)) != 15 and len(str(consequent)) != 15:  \n",
    "                                          \n",
    "                                    new_rules.append(rule)\n",
    "\n",
    "                        new_rules_df = pd.DataFrame(new_rules)\n",
    "                        rule_list = new_rules_df.to_dict(orient='records')\n",
    "                        rule_df = pd.DataFrame(rule_list)\n",
    "                        rule_df = rule_df.loc[:, ['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "                        rule_df.to_csv(p+'/'+project+'/New_IaC_Rules_Apriori.csv')\n",
    "except FileNotFoundError:\n",
    "            \n",
    "            pass\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
